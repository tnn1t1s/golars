# golars Benchmarks

This directory contains the **complete Polars benchmark suite** implemented in golars.

## Polars Source

All benchmarks are direct implementations of the Polars test suite:
- **Source**: https://github.com/pola-rs/polars/tree/main/py-polars/tests/benchmark

## Complete Benchmark Mapping

### test_group_by.py

| Polars Test | golars Benchmark | Status |
|-------------|------------------|--------|
| `test_groupby_h2oai_q1` | `BenchmarkGroupByH2OAI_Q1` | Comparable |
| `test_groupby_h2oai_q2` | `BenchmarkGroupByH2OAI_Q2` | Comparable |
| `test_groupby_h2oai_q3` | `BenchmarkGroupByH2OAI_Q3` | Comparable |
| `test_groupby_h2oai_q4` | `BenchmarkGroupByH2OAI_Q4` | Comparable |
| `test_groupby_h2oai_q5` | `BenchmarkGroupByH2OAI_Q5` | Comparable |
| `test_groupby_h2oai_q6` | `BenchmarkGroupByH2OAI_Q6` | Comparable |
| `test_groupby_h2oai_q7` | `BenchmarkGroupByH2OAI_Q7` | **SKIPPED** |
| `test_groupby_h2oai_q8` | `BenchmarkGroupByH2OAI_Q8` | Comparable |
| `test_groupby_h2oai_q9` | `BenchmarkGroupByH2OAI_Q9` | Comparable |
| `test_groupby_h2oai_q10` | `BenchmarkGroupByH2OAI_Q10` | Comparable |

### test_filter.py

| Polars Test | golars Benchmark | Status |
|-------------|------------------|--------|
| `test_filter1` | `BenchmarkFilter1` | Comparable |
| `test_filter2` | `BenchmarkFilter2` | Comparable |

### test_io.py

| Polars Test | golars Benchmark | Status |
|-------------|------------------|--------|
| `test_write_read_scan_large_csv` | `BenchmarkWriteReadFilterCSV` | Comparable (lazy scan_csv implemented) |

### test_join_where.py

| Polars Test | golars Benchmark | Status |
|-------------|------------------|--------|
| `test_strict_inequalities` | `BenchmarkStrictInequalities` | Comparable |
| `test_non_strict_inequalities` | `BenchmarkNonStrictInequalities` | Comparable |
| `test_single_inequality` | `BenchmarkSingleInequality` | Comparable |

### test_with_columns.py

| Polars Test | golars Benchmark | Status |
|-------------|------------------|--------|
| `test_with_columns_quadratic_19503` | `BenchmarkWithColumnsQuadratic` | Comparable |

### interop/test_numpy.py

| Polars Test | golars Benchmark | Status |
|-------------|------------------|--------|
| `test_to_numpy_series_zero_copy` | `BenchmarkToNumpySeriesZeroCopy` | **SKIPPED** |
| `test_to_numpy_series_with_nulls` | `BenchmarkToNumpySeriesWithNulls` | **SKIPPED** |
| `test_to_numpy_series_chunked` | `BenchmarkToNumpySeriesChunked` | **SKIPPED** |

## Feature Gaps

Skipped benchmarks clearly show golars feature gaps:

| Feature | golars Status | Polars Equivalent |
|---------|---------------|-------------------|
| Expression arithmetic in agg | Missing | `pl.max() - pl.min()` |
| top_k aggregation | Missing | `col.top_k(2)` |
| NumPy interop | N/A (Go) | `.to_numpy()` |
| Lazy evaluation | Partial | `scan_csv().filter().collect()` |

## Running Benchmarks

```bash
# Optional: override where benchmark parquet files are stored
export GOLARS_BENCH_DATA_DIR=/path/to/benchmarks/data

# Run ALL Polars-equivalent benchmarks
go test -bench=. ./benchmarks/... -benchmem

# Run specific suite
go test -bench=. ./benchmarks/groupby -benchmem
go test -bench=. ./benchmarks/filter -benchmem
go test -bench=. ./benchmarks/io -benchmem
go test -bench=. ./benchmarks/join_where -v  # Shows skipped tests
go test -bench=. ./benchmarks/with_columns -benchmem
go test -bench=. ./benchmarks/interop -v  # Shows skipped tests

# Show all skipped benchmarks (feature gaps)
go test -bench=. ./benchmarks/... -v 2>&1 | grep -E "SKIP|FEATURE GAP|LANGUAGE GAP"
```

## Benchmark Invariants

To keep comparisons fair, all cross-engine benchmarks must use:
- the same dataset file (Parquet generated by golars),
- the same query definitions,
- the same row counts and schema.

The shared suite runner (`benchmarks/compare/suite.json`) enforces this by:
- loading the same Parquet file in both Go and Polars,
- using identical query specs in both runners,
- emitting dataset hashes in the result JSON for verification.

## Suite Runner (Golars vs Polars)

```bash
cd benchmarks
just generate-light-data
just compare-suite
```

Outputs:
- `benchmarks/compare/results/golars_suite.json`
- `benchmarks/compare/results/polars_suite.json`

Analyze results:

```bash
cd benchmarks/compare
python3 analyze_suite.py --golars results/golars_suite.json --polars results/polars_suite.json
```

## Directory Structure

```
benchmarks/
├── groupby/       # test_group_by.py (Q1-Q10)
├── filter/        # test_filter.py
├── io/            # test_io.py
├── join_where/    # test_join_where.py (all skipped)
├── with_columns/  # test_with_columns.py
├── interop/       # interop/test_numpy.py (all skipped)
├── data/          # H2O.ai data generation
├── agg/           # Additional aggregation benchmarks
├── sort/          # Additional sort benchmarks
└── join/          # Additional equality join benchmarks
```

## Data Configuration

Matches Polars `conftest.py`:

```python
# Polars
groupby_data = generate_group_by_data(10_000, 100, null_ratio=0.05)
```

```go
// golars
H2OAISmall = H2OAIConfig{
    NRows:     10_000,
    NGroups:   100,
    NullRatio: 0.05,
}
```

## Running Polars Benchmarks

To get Polars numbers for comparison:

```bash
cd /path/to/polars/py-polars
pytest tests/benchmark/ --benchmark-only -v
```
