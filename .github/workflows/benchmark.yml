name: Benchmark Report

on:
  pull_request:
    branches: [main]

jobs:
  scorecard:
    name: Three-tier scorecard
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'

      - name: Verify benchmark data
        run: |
          if [ ! -f benchmarks/data/h2oai_small.parquet ]; then
            echo "::error::benchmarks/data/h2oai_small.parquet missing from repo"
            exit 1
          fi

      - name: Run scorecard
        id: scorecard
        run: bash benchmarks/scorecard.sh

      - name: Parse scorecard for PR comment
        id: parse
        if: always() && hashFiles('scorecard.json') != ''
        run: |
          # Extract values from scorecard.json using jq
          BUILD=$(jq -r '.tiers.t1_correctness.build' scorecard.json)
          RACE=$(jq -r '.tiers.t1_correctness.race' scorecard.json)
          T1_PASS=$(jq '.tiers.t1_correctness.tests.pass' scorecard.json)
          T1_FAIL=$(jq '.tiers.t1_correctness.tests.fail' scorecard.json)
          T1_SKIP=$(jq '.tiers.t1_correctness.tests.skip' scorecard.json)
          T1_TOTAL=$(jq '.tiers.t1_correctness.tests.total' scorecard.json)
          T1_RATE=$(jq '.tiers.t1_correctness.pass_rate' scorecard.json)
          PKG_PASS=$(jq '.tiers.t1_correctness.packages.pass' scorecard.json)
          PKG_FAIL=$(jq '.tiers.t1_correctness.packages.fail' scorecard.json)

          T2_PASS=$(jq '.tiers.t2_runability.benchmarks.pass' scorecard.json)
          T2_FAIL=$(jq '.tiers.t2_runability.benchmarks.fail' scorecard.json)
          T2_SKIP=$(jq '.tiers.t2_runability.benchmarks.skip' scorecard.json)
          T2_TOTAL=$(jq '.tiers.t2_runability.benchmarks.total' scorecard.json)
          T2_RATE=$(jq '.tiers.t2_runability.run_rate' scorecard.json)

          T3_COUNT=$(jq '.tiers.t3_performance.benchmarks | length' scorecard.json)
          T3_GEO=$(jq '.tiers.t3_performance.geometric_mean_ratio' scorecard.json)

          # Save as outputs
          {
            echo "build=$BUILD"
            echo "race=$RACE"
            echo "t1_pass=$T1_PASS"
            echo "t1_fail=$T1_FAIL"
            echo "t1_skip=$T1_SKIP"
            echo "t1_total=$T1_TOTAL"
            echo "t1_rate=$T1_RATE"
            echo "pkg_pass=$PKG_PASS"
            echo "pkg_fail=$PKG_FAIL"
            echo "t2_pass=$T2_PASS"
            echo "t2_fail=$T2_FAIL"
            echo "t2_skip=$T2_SKIP"
            echo "t2_total=$T2_TOTAL"
            echo "t2_rate=$T2_RATE"
            echo "t3_count=$T3_COUNT"
            echo "t3_geo=$T3_GEO"
          } >> "$GITHUB_OUTPUT"

      - name: Build T3 performance table
        id: t3table
        if: always() && hashFiles('scorecard.json') != ''
        run: |
          # Build a markdown table of T3 results (top 20 by ratio, if baseline exists)
          jq -r '
            .tiers.t3_performance.benchmarks
            | map(select(.vs_reference != null))
            | sort_by(.vs_reference) | reverse
            | .[:20]
            | ["| Benchmark | ns/op | vs ref |", "|-----------|------:|-------:|"]
              + [.[] | "| `\(.name)` | \(.ns_op) | \(.vs_reference)x |"]
            | .[]
          ' scorecard.json > t3-table.md 2>/dev/null || echo "_No reference comparisons available._" > t3-table.md

      - name: Post PR comment
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            const build = '${{ steps.parse.outputs.build }}' || 'unknown';
            const race = '${{ steps.parse.outputs.race }}' || 'unknown';

            const t1Pass = '${{ steps.parse.outputs.t1_pass }}' || '0';
            const t1Fail = '${{ steps.parse.outputs.t1_fail }}' || '0';
            const t1Skip = '${{ steps.parse.outputs.t1_skip }}' || '0';
            const t1Total = '${{ steps.parse.outputs.t1_total }}' || '0';
            const t1Rate = '${{ steps.parse.outputs.t1_rate }}' || '0.0';
            const pkgPass = '${{ steps.parse.outputs.pkg_pass }}' || '0';
            const pkgFail = '${{ steps.parse.outputs.pkg_fail }}' || '0';

            const t2Pass = '${{ steps.parse.outputs.t2_pass }}' || '0';
            const t2Fail = '${{ steps.parse.outputs.t2_fail }}' || '0';
            const t2Skip = '${{ steps.parse.outputs.t2_skip }}' || '0';
            const t2Total = '${{ steps.parse.outputs.t2_total }}' || '0';
            const t2Rate = '${{ steps.parse.outputs.t2_rate }}' || '0.0';

            const t3Count = '${{ steps.parse.outputs.t3_count }}' || '0';
            const t3Geo = '${{ steps.parse.outputs.t3_geo }}' || 'null';

            const icon = (s) => s === 'pass' ? ':white_check_mark:' : (s === 'skip' ? ':fast_forward:' : ':x:');

            let t3Table = '';
            try { t3Table = fs.readFileSync('t3-table.md', 'utf8'); } catch (e) { t3Table = '_T3 data unavailable._'; }

            const t3GeoDisplay = t3Geo === 'null' ? 'N/A (no baseline)' : `${t3Geo}x`;

            const body = `## Three-Tier Benchmark Scorecard

### T1: Correctness

| Metric | Result |
|--------|--------|
| **Build** | ${icon(build)} ${build} |
| **Race detector** | ${icon(race)} ${race} |
| **Tests** | ${t1Pass} / ${t1Total} passing (${t1Rate}%) |
| **Failing** | ${t1Fail} |
| **Skipped** | ${t1Skip} |
| **Packages** | ${pkgPass} pass, ${pkgFail} fail |

### T2: Benchmark Runability

| Metric | Result |
|--------|--------|
| **Benchmarks passing** | ${t2Pass} / ${t2Total} (${t2Rate}%) |
| **Benchmarks failing** | ${t2Fail} |
| **Benchmarks skipped** | ${t2Skip} |

### T3: Performance vs Reference

| Metric | Result |
|--------|--------|
| **Benchmarks measured** | ${t3Count} |
| **Geometric mean ratio** | ${t3GeoDisplay} |

<details>
<summary>Performance detail (top 20 by ratio)</summary>

${t3Table}

_Ratio = agent_ns / reference_ns. Lower is better; 1.0 = same as reference._
</details>

---
_Generated by \`benchmarks/scorecard.sh\` | [Download scorecard.json](../actions/runs/${{ github.run_id }})_`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const marker = 'Three-Tier Benchmark Scorecard';
            const existing = comments.find(c => c.body.includes(marker));

            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body,
              });
            }

      - name: Upload scorecard artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scorecard
          path: |
            scorecard.json
          retention-days: 90
